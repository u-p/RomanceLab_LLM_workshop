{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc87d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minicons\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install transformers\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5606d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \"scorer\" module from the minicons library\n",
    "from minicons import scorer\n",
    "\n",
    "# Load helper functions from the file helper_functions.py\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b37e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "# Load the specified model IncrementalLMScorer is a wrapper around any causal (autoregressive) language model accessible via Hugging Faceâ€™s transformers.\n",
    "lmScorer = scorer.IncrementalLMScorer(model_name)\n",
    "\n",
    "BOS = initialize_bos(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokenizer for calculating word-level surprisals\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "word_tokenizer = TweetTokenizer().tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_sg = \"The key to the cabinet was rusty from many years of disuse.\"\n",
    "gram_pl = \"The key to the cabinets was rusty from many years of disuse.\"\n",
    "ungram_sg = \"The key to the cabinet were rusty from many years of disuse.\"\n",
    "ungram_pl = \"The key to the cabinets were rusty from many years of disuse.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa8ef6",
   "metadata": {},
   "source": [
    "#### Generate word-by-word surprisals for `Agreement Attraction` experiment in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_surprisal_gram_sg = lmScorer.word_score_tokenized(\n",
    "    gram_sg, \n",
    "    bos_token=BOS,\n",
    "    tokenize_function=word_tokenizer,\n",
    "    surprisal=True,\n",
    "    bow_correction=True,\n",
    ")\n",
    "\n",
    "plot_word_by_word_surprisals(word_surprisal_gram_sg)\n",
    "\n",
    "word_surprisal_gram_pl = lmScorer.word_score_tokenized(\n",
    "    gram_pl, \n",
    "    bos_token=BOS,\n",
    "    tokenize_function=word_tokenizer,\n",
    "    surprisal=True,\n",
    "    bow_correction=True,\n",
    ")\n",
    "\n",
    "plot_word_by_word_surprisals(word_surprisal_gram_pl)\n",
    "\n",
    "word_surprisal_ungram_sg = lmScorer.word_score_tokenized(\n",
    "    ungram_sg, \n",
    "    bos_token=BOS,\n",
    "    tokenize_function=word_tokenizer,\n",
    "    surprisal=True,\n",
    "    bow_correction=True,\n",
    ")\n",
    "\n",
    "plot_word_by_word_surprisals(word_surprisal_ungram_sg)\n",
    "\n",
    "word_surprisal_ungram_pl = lmScorer.word_score_tokenized(\n",
    "    ungram_pl, \n",
    "    bos_token=BOS,\n",
    "    tokenize_function=word_tokenizer,\n",
    "    surprisal=True,\n",
    "    bow_correction=True,\n",
    ")\n",
    "\n",
    "plot_word_by_word_surprisals(word_surprisal_ungram_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab53dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_word_surprisal\n",
    "\n",
    "gram_sg = \"The key to the cabinet was rusty from many years of disuse.\"\n",
    "gram_pl = \"The key to the cabinets was rusty from many years of disuse.\"\n",
    "ungram_sg = \"The key to the cabinet were rusty from many years of disuse.\"\n",
    "ungram_pl = \"The key to the cabinets were rusty from many years of disuse.\"\n",
    "\n",
    "target_word = \"was\"\n",
    "word_surp = get_word_surprisal(scorer=lmScorer, BOS=BOS, sentence=gram_sg, target=target_word)\n",
    "print(f\"Surprisal for \\'{target_word}\\' in \\'{gram_sg}\\'\\n{word_surp}\\n\")\n",
    "\n",
    "target_word = \"was\"\n",
    "word_surp = get_word_surprisal(scorer=lmScorer, BOS=BOS, sentence=gram_pl, target=target_word)\n",
    "print(f\"Surprisal for \\'{target_word}\\' in \\'{gram_pl}\\'\\n{word_surp}\\n\")\n",
    "\n",
    "target_word = \"were\"\n",
    "word_surp = get_word_surprisal(scorer=lmScorer, BOS=BOS, sentence=ungram_sg, target=target_word)\n",
    "print(f\"Surprisal for \\'{target_word}\\' in \\'{ungram_sg}\\'\\n{word_surp}\\n\")\n",
    "\n",
    "target_word = \"were\"\n",
    "word_surp = get_word_surprisal(scorer=lmScorer, BOS=BOS, sentence=ungram_pl, target=target_word)\n",
    "print(f\"Surprisal for \\'{target_word}\\' in \\'{ungram_pl}\\'\\n{word_surp}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psycholinx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
